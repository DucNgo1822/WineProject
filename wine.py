# -*- coding: utf-8 -*-
"""Wine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LKwUDYBsGtshsAIWFmdhCrhSTNt2eLGR
"""

import pandas as pd

redwine= pd.read_csv("WineQualityRed.csv")

whitewine = pd.read_csv("WineQualityWhite.csv")

from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve
from sklearn.metrics import confusion_matrix
import numpy as np

full = pd.concat([redwine,whitewine], ignore_index = True)
full.head()



corr = full.corr()
fig, ax = plt.subplots(figsize = (10,10))
g= sns.heatmap(corr,ax=ax, annot= True)
ax.set_title('Correlation between variables')

"""# Using all quality points as labels (7 labels)

Firstly, we want to test the case using all 7 labels (each corresponds with a quality point) of the response variable (quality) for making predictions
"""

corr_vec = full.corr()['quality']
print(abs(corr_vec) > 0.05)
print((abs(corr_vec)).idxmin())

"""Since there are some factors have weak correlation with the response variable (quality points), we remove the 4 factors with lowest correlation"""

# Mapping the quality in the correct order from 0 - 6
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
full['quality'] = le.fit_transform(full['quality'])
print(full['quality'].unique())
full.reset_index()

# Rescalling the data with min-max Scale
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

full_omit = full.drop(['residual sugar','pH','sulphates','total sulfur dioxide','quality'], axis = 1)

scaler.fit(full_omit)
full_omit_scaled = scaler.transform(full_omit)

# Splitting train and test set
X_train_full1, X_test_full1,Y_train_full1,Y_test_full1 = train_test_split(full_omit_scaled,full['quality'], test_size = 0.1, random_state = 100)
X_train_full2, X_test_full2,Y_train_full2, Y_test_full2 = train_test_split(full_omit_scaled,full['quality'], test_size = 0.1, random_state = 220)
X_train_full3, X_test_full3,Y_train_full3,Y_test_full3 = train_test_split(full_omit_scaled,full['quality'], test_size = 0.1, random_state = 343)

train_full = [X_train_full1, X_train_full2, X_train_full3]
test_full = [X_test_full1, X_test_full2, X_test_full3]
Y_train_full = [Y_train_full1,Y_train_full2,Y_train_full3]
Y_test_full = [Y_test_full1, Y_test_full2, Y_test_full3]

# Using K-means Clustering
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report,confusion_matrix
#define model
pred = []
for i in range(len(train_full)):
  model_kmeans = KMeans(n_clusters=7, random_state=1).fit(train_full[i])
  pred.append(model_kmeans.predict(test_full[i]))
  print(classification_report(Y_test_full[i], pred[i]))

from sklearn.linear_model import LogisticRegression
# Using Softmax Regression

pred = []
for i in range(len(train_full)):
  softmax_reg = LogisticRegression(C=1e5, solver = 'lbfgs', multi_class = 'multinomial').fit(train_full[i],Y_train_full[i])
  pred.append(softmax_reg.predict(test_full[i]))
  print("Classification Report for model using dataset {}".format(i+1))
  print(classification_report(Y_test_full[i], pred[i]))

# Using Linear Regression (as requested of the problem set)

from sklearn.linear_model import LinearRegression

pred = []
for i in range(len(train_full)):
  linear_reg = LinearRegression().fit(train_full[i],Y_train_full[i])
  print("R-squared for model using dataset {}".format(i+1))
  print(linear_reg.score(test_full[i],Y_test_full[i]))

"""**Regrouping Quality Points**

We try again by grouping into only 3 labels (solve the problem for the unbalance distribution of labels). We split quality points into 3 groups : quality points <= 5 (bad); 5 < quality points <= 7 (normal); and quality points = 8 or 9 (good)
"""

full = pd.concat([redwine,whitewine], ignore_index = True)
full.quality.unique()

# PLotting the distributions of the quality points (orignial)
plt.hist(full.quality.values,bins=[2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5],edgecolor = 'black')
plt.xlabel('Wine quality', fontsize = 18)
plt.ylabel('Counts', fontsize = 18)
plt.show()

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

full_omit = full.drop(['residual sugar','pH','sulphates','total sulfur dioxide','quality'], axis = 1)

scaler.fit(full_omit)
full_omit_scaled = scaler.transform(full_omit)
full_omit.head()

plt.scatter(full_omit['fixed acidity'], full.quality,  color='blue')
plt.xlabel("fixed acidity")
plt.ylabel("Quality")
plt.show()

plt.scatter(full_omit['volatile acidity'], full.quality,  color='blue')
plt.xlabel("volatile acidity")
plt.ylabel("Quality")
plt.show()

plt.scatter(full_omit['citric acid'], full.quality,  color='blue')
plt.xlabel("citric acid")
plt.ylabel("Quality")
plt.show()

plt.scatter(full_omit['chlorides'], full.quality,  color='blue')
plt.xlabel("chlorides")
plt.ylabel("Quality")
plt.show()

plt.scatter(full_omit['alcohol'], full.quality,  color='blue')
plt.xlabel("alcohol")
plt.ylabel("Quality")
plt.show()

plt.scatter(full_omit['density'], full.quality,  color='blue')
plt.xlabel("density")
plt.ylabel("Quality")
plt.show()

plt.scatter(full_omit['free sulfur dioxide'], full.quality,  color='blue')
plt.xlabel("free sulfur dioxide")
plt.ylabel("Quality")
plt.show()

def group_quality (col):
  quality = col[0]
  if quality <=5:
    return 0
  elif quality == 7 or quality == 6:
    return 1
  else: 
    return 2
full['quality'] = full[['quality']].apply(group_quality, axis = 1)
full.quality.value_counts()

X_train_full1, X_test_full1,Y_train_full1,Y_test_full1 = train_test_split(full_omit_scaled,full['quality'], test_size = 0.1, random_state = 100)
X_train_full2, X_test_full2,Y_train_full2, Y_test_full2 = train_test_split(full_omit_scaled,full['quality'], test_size = 0.1, random_state = 220)
X_train_full3, X_test_full3,Y_train_full3,Y_test_full3 = train_test_split(full_omit_scaled,full['quality'], test_size = 0.1, random_state = 343)

train_full = [X_train_full1, X_train_full2, X_train_full3]
test_full = [X_test_full1, X_test_full2, X_test_full3]
Y_train_full = [Y_train_full1,Y_train_full2,Y_train_full3]
Y_test_full = [Y_test_full1, Y_test_full2, Y_test_full3]

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Using K-means Clustering
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report,confusion_matrix
import itertools

#from sklearn.metrics import plot_confusion_matrix
#define model
pred = []
for i in range(len(train_full)):
  model_kmeans = KMeans(n_clusters=3, random_state=1).fit(train_full[i])
  pred.append(model_kmeans.predict(test_full[i]))
  print(classification_report(Y_test_full[i], pred[i]))
  print(confusion_matrix(Y_test_full[i], pred[i]))
  #plot_confusion_matrix(confusion_matrix(Y_test_full[0], pred[0]),classes=['quality(0)','quality(1)', 'quality(2)'],normalize=False)

plot_confusion_matrix(confusion_matrix(Y_test_full[0], pred[0]),classes=['quality(0)','quality(1)', 'quality(2)'],normalize=False)

plot_confusion_matrix(confusion_matrix(Y_test_full[1], pred[1]),classes=['quality(0)','quality(1)', 'quality(2)'],normalize=False)

plot_confusion_matrix(confusion_matrix(Y_test_full[2],pred[2]),classes=['quality(0)','quality(1)', 'quality(2)'],normalize=False)

"""Although there is an increase in the average accuracy (0.28 compared to 0.17) when applying KMeans (k = 3) Clustering Algorithms after regrouping, the variance of this model is quite high, in which might cause the problem of overfitting. Thus, with such low accuracy and high variance, this is not a good approach when using KMeans Clustering Algorithm to solve the problem."""

from sklearn.linear_model import LogisticRegression
# Using Softmax Regression

softmax_reg = LogisticRegression(C=1e5, solver = 'lbfgs', multi_class = 'multinomial')
pred = []
for i in range(len(train_full)):
  softmax_reg = LogisticRegression(C=1e5, solver = 'lbfgs', multi_class = 'multinomial').fit(train_full[i],Y_train_full[i])
  pred.append(softmax_reg.predict(test_full[i]))
  print("Classification Report for model using dataset {}".format(i+1))
  print(classification_report(Y_test_full[i], pred[i]))

"""Thus, we can see that using softmax regression for the dataset after regrouping the quality points has increased, with the average accuracy for three different pairs of trainning and test set is about 71% percent, along with low variance."""

